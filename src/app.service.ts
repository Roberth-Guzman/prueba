import { Injectable } from '@nestjs/common';
import { PrismaService } from './prisma/prisma.service';
import { LlmService } from './llm/llm.service';

@Injectable()
export class AppService {
  constructor(
    private readonly prisma: PrismaService,
    private readonly llm: LlmService,
  ) {}

  async chat(message: string): Promise<string> {
    if (!message || message.trim() === '') {
      return 'Por favor escribe o di el nombre de un medicamento.';
    }

    // 游댳 1. Clasificar la intenci칩n del usuario y extraer el medicamento
    const { intent, medication } =
      await this.llm.classifyIntentAndExtractMedication(message);

    // 游댳 2. Actuar seg칰n la intenci칩n
    switch (intent) {
      case 'SPECIFIC_QUERY':
        // Si la IA no pudo extraer un medicamento, pedimos al usuario que sea m치s espec칤fico.
        if (!medication) {
          return 'No entend칤 qu칠 medicamento buscas. Por favor, intenta de nuevo con el nombre del producto.';
        }

        // Buscar el medicamento en la base de datos
        const medicamentos = await this.prisma.medicamento.findMany({
          where: {
            nombre: { contains: medication },
          },
        });

        // Usar el LlmService para generar la respuesta de encontrado o no encontrado
        if (medicamentos.length > 0) {
          return this.llm.generateFoundResponse(message, medicamentos);
        } else {
          return this.llm.generateNotFoundResponse(medication); // Pasamos el medicamento extra칤do
        }

      case 'OTHER':
        // Para cualquier otra intenci칩n, dejamos que la IA genere una respuesta conversacional
        return this.llm.generateGeneralResponse(message);

      default:
        // Fallback por si la IA devuelve una intenci칩n no esperada
        return this.llm.generateGeneralResponse(message);
    }
  }
}